{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-8UCN0QB:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataTransformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2edc0058ca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataTransformations\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+-----+-------+------+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|PRICE|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+-----+-------+------+-----+-----+\n",
      "|0.00632|18.0| 2.31| 0.0|0.538|6.575|65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07| 0.0|0.469|6.421|78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14| 21.6|\n",
      "|0.02729| 0.0| 7.07| 0.0|0.469|7.185|61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03| 34.7|\n",
      "|0.03237| 0.0| 2.18| 0.0|0.458|6.998|45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94| 33.4|\n",
      "|0.06905| 0.0| 2.18| 0.0|0.458|7.147|54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33| 36.2|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+-----+-------+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'Boston_House_Prices.csv'\n",
    "df = spark.read.csv(path,header=True,inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(),len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM\n",
      "ZN\n",
      "INDUS\n",
      "CHAS\n",
      "NOX\n",
      "RM\n",
      "AGE\n",
      "DIS\n",
      "RAD\n",
      "TAX\n",
      "PTRATIO\n",
      "B\n",
      "LSTAT\n",
      "PRICE\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+-----+\n",
      "|CRIM| ZN|INDUS|CHAS|NOX| RM|AGE|DIS|RAD|TAX|PTRATIO|  B|LSTAT|PRICE|\n",
      "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+-----+\n",
      "|   0|  0|    0|   0|  0|  0|  0|  0|  0|  0|      0|  0|    0|    0|\n",
      "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eksik değerler kontrolü\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df içindeki eksik değerleri kontrol et\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|              CRIM|                ZN|             INDUS|              CHAS|                NOX|                RM|               AGE|              DIS|              RAD|               TAX|           PTRATIO|                 B|             LSTAT|             PRICE|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               506|               506|               506|               506|                506|               506|               506|              506|              506|               506|               506|               506|               506|               506|\n",
      "|   mean|3.6135235573122535|11.363636363636363|11.136778656126504|0.0691699604743083| 0.5546950592885372| 6.284634387351787| 68.57490118577078|3.795042687747034|9.549407114624506| 408.2371541501976|18.455533596837967|356.67403162055257|12.653063241106723|22.532806324110698|\n",
      "| stddev| 8.601545105332491| 23.32245299451514| 6.860352940897589|0.2539940413404101|0.11587767566755584|0.7026171434153232|28.148861406903595| 2.10571012662761|8.707259384239366|168.53711605495903|2.1649455237144455| 91.29486438415782| 7.141061511348571| 9.197104087379815|\n",
      "|    min|           0.00632|               0.0|              0.46|               0.0|              0.385|             3.561|               2.9|           1.1296|              1.0|             187.0|              12.6|              0.32|              1.73|               5.0|\n",
      "|    max|           88.9762|             100.0|             27.74|               1.0|              0.871|              8.78|             100.0|          12.1265|             24.0|             711.0|              22.0|             396.9|             37.97|              50.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubleType():\n",
      "CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT, PRICE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame şemasını al\n",
    "schema = df.schema\n",
    "\n",
    "# Sütunları veri tipine göre gruplandır\n",
    "grouped_columns = {}\n",
    "for field in schema.fields:\n",
    "    data_type = str(field.dataType)\n",
    "    column_name = field.name\n",
    "    grouped_columns.setdefault(data_type, []).append(column_name)\n",
    "\n",
    "# Gruplandırılmış sütunları yazdır\n",
    "for data_type, columns in grouped_columns.items():\n",
    "    column_list = \", \".join(columns)\n",
    "    print(f\"{data_type}:\\n{column_list}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.00632,18.0,2.3...| 24.0|\n",
      "|[0.02731,0.0,7.07...| 21.6|\n",
      "|[0.02729,0.0,7.07...| 34.7|\n",
      "|[0.03237,0.0,2.18...| 33.4|\n",
      "|[0.06905,0.0,2.18...| 36.2|\n",
      "|[0.02985,0.0,2.18...| 28.7|\n",
      "|[0.08829,12.5,7.8...| 22.9|\n",
      "|[0.14455,12.5,7.8...| 27.1|\n",
      "|[0.21124,12.5,7.8...| 16.5|\n",
      "|[0.17004,12.5,7.8...| 18.9|\n",
      "|[0.22489,12.5,7.8...| 15.0|\n",
      "|[0.11747,12.5,7.8...| 18.9|\n",
      "|[0.09378,12.5,7.8...| 21.7|\n",
      "|[0.62976,0.0,8.14...| 20.4|\n",
      "|[0.63796,0.0,8.14...| 18.2|\n",
      "|[0.62739,0.0,8.14...| 19.9|\n",
      "|[1.05393,0.0,8.14...| 23.1|\n",
      "|[0.7842,0.0,8.14,...| 17.5|\n",
      "|[0.80271,0.0,8.14...| 20.2|\n",
      "|[0.7258,0.0,8.14,...| 18.2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# features kolonunu oluşturma\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "x_columns = df.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=x_columns, outputCol='features')\n",
    "output = assembler.transform(df)\n",
    "\n",
    "\n",
    "df = output.select('features', output['PRICE'].alias('label'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.00632,18.0,2.3...| 24.0|\n",
      "|[0.00906,90.0,2.9...| 32.2|\n",
      "|[0.01301,35.0,1.5...| 32.7|\n",
      "|[0.01311,90.0,1.2...| 35.4|\n",
      "|[0.0136,75.0,4.0,...| 18.9|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.01096,55.0,2.2...| 22.0|\n",
      "|[0.01381,80.0,0.4...| 50.0|\n",
      "|[0.01439,60.0,2.9...| 29.1|\n",
      "|[0.01778,95.0,1.4...| 32.9|\n",
      "|[0.02177,82.5,2.0...| 42.3|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor()\n",
    "gbt.setSeed(42)\n",
    "\n",
    "# Parametre grid'i oluştur\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(gbt.maxDepth, [5, 10, 15])\n",
    "              .addGrid(gbt.maxBins, [20, 30, 40])\n",
    "              .build())\n",
    "\n",
    "# Cross-validation nesnesini oluştur\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Modeli eğit\n",
    "cv_model = crossval.fit(train)\n",
    "\n",
    "# En iyi modeli seç\n",
    "model = cv_model.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTAT: 0.3633204950480282\n",
      "RM: 0.20350629813268223\n",
      "DIS: 0.09563236343962037\n",
      "AGE: 0.077184290037895\n",
      "TAX: 0.05994216127320513\n",
      "CRIM: 0.04861801926322124\n",
      "B: 0.038291070078874744\n",
      "NOX: 0.032101111498420536\n",
      "PTRATIO: 0.030945416360422414\n",
      "INDUS: 0.021439798127227485\n",
      "RAD: 0.021430720159297766\n",
      "ZN: 0.00417554085125234\n",
      "CHAS: 0.0034127157298525232\n"
     ]
    }
   ],
   "source": [
    "feature_Importances = list(model.featureImportances)\n",
    "feats = output.columns[:-2]\n",
    "d = dict(zip(feats, feature_Importances))\n",
    "sorted_dict = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Sıralı sözlüğü yazdırma\n",
    "for feature, importance in sorted_dict.items():\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train seti üzerinde tahminler yapın\n",
    "train_predictions = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train seti\n",
      "RMSE = 1.28325\n",
      "R Squared (R2) = 0.979583\n",
      "MSE = 1.64674\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"Train seti\")\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "print(\"RMSE = %g\" % evaluator.evaluate(train_predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "print(\"R Squared (R2) = %g\" % evaluator.evaluate(train_predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"mse\")\n",
    "print(\"MSE = %g\" % evaluator.evaluate(train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.01096,55.0,2.2...| 22.0|\n",
      "|[0.01381,80.0,0.4...| 50.0|\n",
      "|[0.01439,60.0,2.9...| 29.1|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[0.01096,55.0,2.2...| 22.0| 24.20113362991416|\n",
      "|[0.01381,80.0,0.4...| 50.0| 47.82411261237669|\n",
      "|[0.01439,60.0,2.9...| 29.1|25.871528653909127|\n",
      "|[0.01778,95.0,1.4...| 32.9| 33.15614853367658|\n",
      "|[0.02177,82.5,2.0...| 42.3| 47.69170633525798|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test seti\n",
      "RMSE = 2.88197\n",
      "R Squared (R2) = 0.921285\n",
      "MSE = 8.30575\n",
      "\n",
      "Train seti\n",
      "RMSE = 1.28325\n",
      "R Squared (R2) = 0.979583\n",
      "MSE = 1.64674\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"Test seti\")\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "print(\"RMSE = %g\" % evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "print(\"R Squared (R2) = %g\" % evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"mse\")\n",
    "print(\"MSE = %g\" % evaluator.evaluate(predictions))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Train seti\")\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "print(\"RMSE = %g\" % evaluator.evaluate(train_predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "print(\"R Squared (R2) = %g\" % evaluator.evaluate(train_predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"mse\")\n",
    "print(\"MSE = %g\" % evaluator.evaluate(train_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
